{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Symplegma \u00b6 Symplegma (from greek \u03c3\u03cd\u03bc\u03c0\u03bb\u03b5\u03b3\u03bc\u03b1 ) is a simple set of Ansible playbooks to deploy Kubernetes with Kubeadm . It is heavily inspired by Kubespray and OpenStack Ansible . Symplegma is Kubernetes certified since v1.12 . Check out CNCF Landscape . The main goal is to be minimalist with sensible defaults. starting in v2, ansible role support for CNI plugin has been removed, as most commonly used plugin use straight foward deployment process. To migrate to tigera operator please see this guide . Support to install more CNI with native installation might be added in the futur. Deploys a Kubernetes cluster \u00b6 Deploys vanilla Kubernetes with Kubeadm. Supports Flatcar Linux / Ubuntu 20.04 Does not rely on Docker Uses CRI compatible runtime: containerd (default) cri-o Does not depend on cloud provider Does not depend on primary master Dynamic config Always up to date: No deprecated options Documentation \u00b6 Documentation is generated using mkdocs and the sources are located in the ./docs/ directory. It is available online at particuleio.github.io/symplegma . Roles \u00b6 symplegma-os_bootstrap : Configure the hosts OS to support Vanilla Kubernetes symplegma-kubernetes_hosts : Bootstrap Kubernetes on Linux hosts symplegma-win_kubernetes_hosts : Bootstrap Kubernetes on Windows hosts symplegma-kubeadm : Bootstrap the Kubernetes Cluster using kubeadm symplegma-containerd : Install the containerd CRI symplegma-crio : Install the cri-o CRI symplegma-win_docker : Install the [docker][cri-docker] CRI on Windows hosts symplegma-cni : Boostrap the hosts to install the CNI symplegma-flannel : Bootstrap and install the Flannel CNI symplegma-win_cni : Bootstrap Windows hosts to install the CNI Roadmap \u00b6 Support cilium as network plugin Support Kata container on QEMU and Firecracker Support bootstrapping GitOps Contributing \u00b6 Each role is hosted in a separate repository in particuleio . Exhaustive list of roles can be found in requirements.yml License \u00b6","title":"Overview"},{"location":"#symplegma","text":"Symplegma (from greek \u03c3\u03cd\u03bc\u03c0\u03bb\u03b5\u03b3\u03bc\u03b1 ) is a simple set of Ansible playbooks to deploy Kubernetes with Kubeadm . It is heavily inspired by Kubespray and OpenStack Ansible . Symplegma is Kubernetes certified since v1.12 . Check out CNCF Landscape . The main goal is to be minimalist with sensible defaults. starting in v2, ansible role support for CNI plugin has been removed, as most commonly used plugin use straight foward deployment process. To migrate to tigera operator please see this guide . Support to install more CNI with native installation might be added in the futur.","title":"Symplegma"},{"location":"#deploys-a-kubernetes-cluster","text":"Deploys vanilla Kubernetes with Kubeadm. Supports Flatcar Linux / Ubuntu 20.04 Does not rely on Docker Uses CRI compatible runtime: containerd (default) cri-o Does not depend on cloud provider Does not depend on primary master Dynamic config Always up to date: No deprecated options","title":"Deploys a Kubernetes cluster"},{"location":"#documentation","text":"Documentation is generated using mkdocs and the sources are located in the ./docs/ directory. It is available online at particuleio.github.io/symplegma .","title":"Documentation"},{"location":"#roles","text":"symplegma-os_bootstrap : Configure the hosts OS to support Vanilla Kubernetes symplegma-kubernetes_hosts : Bootstrap Kubernetes on Linux hosts symplegma-win_kubernetes_hosts : Bootstrap Kubernetes on Windows hosts symplegma-kubeadm : Bootstrap the Kubernetes Cluster using kubeadm symplegma-containerd : Install the containerd CRI symplegma-crio : Install the cri-o CRI symplegma-win_docker : Install the [docker][cri-docker] CRI on Windows hosts symplegma-cni : Boostrap the hosts to install the CNI symplegma-flannel : Bootstrap and install the Flannel CNI symplegma-win_cni : Bootstrap Windows hosts to install the CNI","title":"Roles"},{"location":"#roadmap","text":"Support cilium as network plugin Support Kata container on QEMU and Firecracker Support bootstrapping GitOps","title":"Roadmap"},{"location":"#contributing","text":"Each role is hosted in a separate repository in particuleio . Exhaustive list of roles can be found in requirements.yml","title":"Contributing"},{"location":"#license","text":"","title":"License"},{"location":"configuration/container_runtimes/containerd/","text":"Containerd \u00b6 Symplegma supports containerd out of the box with no extra configuration. It is the default runtime. Cgroup configuration \u00b6 To ensure compatibility with older cluster, containerd use cgroupfs as default cgroup. To enable systemd with cgroupv2 on new cluster, simply toggle the variable in group_vars : systemd_cgroup : true","title":"Containerd"},{"location":"configuration/container_runtimes/containerd/#containerd","text":"Symplegma supports containerd out of the box with no extra configuration. It is the default runtime.","title":"Containerd"},{"location":"configuration/container_runtimes/containerd/#cgroup-configuration","text":"To ensure compatibility with older cluster, containerd use cgroupfs as default cgroup. To enable systemd with cgroupv2 on new cluster, simply toggle the variable in group_vars : systemd_cgroup : true","title":"Cgroup configuration"},{"location":"configuration/container_runtimes/crio/","text":"Cri-o \u00b6 Symplegma supports cri-o out of the box. To switch from containerd to cri-o, toggle the variables in group_vars : systemd_cgroup : true container_runtime : crio You must turn on systemd_cgroup in order to use cri-o","title":"CRI-O"},{"location":"configuration/container_runtimes/crio/#cri-o","text":"Symplegma supports cri-o out of the box. To switch from containerd to cri-o, toggle the variables in group_vars : systemd_cgroup : true container_runtime : crio You must turn on systemd_cgroup in order to use cri-o","title":"Cri-o"},{"location":"configuration/kubeadm/kubeadm/","text":"Kubeadm \u00b6 Kubeadm configuration can be overridden in group_vars/all/all/yml As Kubeadm config files are already in YAML, symplegma-kubeadm role just pipes values into the configuration file. This allow you to use all the flags and options possible without Ansible knowing them. The following variable are self explanatory: kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_args: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: {} kubeadm_kubelet_component_config: {} kubeadm_kube_proxy_component_config: {} These variables are added to the Kubeadm config file Example: customize Kubelet \u00b6 kubeadm_kubelet_component_config: | evictionHard: memory.available: \"200Mi\" failSwapOn: \"false\" Check out Kubeadm documentation","title":"Kubeadm"},{"location":"configuration/kubeadm/kubeadm/#kubeadm","text":"Kubeadm configuration can be overridden in group_vars/all/all/yml As Kubeadm config files are already in YAML, symplegma-kubeadm role just pipes values into the configuration file. This allow you to use all the flags and options possible without Ansible knowing them. The following variable are self explanatory: kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_args: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: {} kubeadm_kubelet_component_config: {} kubeadm_kube_proxy_component_config: {} These variables are added to the Kubeadm config file","title":"Kubeadm"},{"location":"configuration/kubeadm/kubeadm/#example-customize-kubelet","text":"kubeadm_kubelet_component_config: | evictionHard: memory.available: \"200Mi\" failSwapOn: \"false\" Check out Kubeadm documentation","title":"Example: customize Kubelet"},{"location":"configuration/networking/ha/","text":"High Availability \u00b6","title":"HA"},{"location":"configuration/networking/ha/#high-availability","text":"","title":"High Availability"},{"location":"configuration/os/flatcar/","text":"Flatcar \u00b6 Here are sensible defaults variables to use when deploying on Flatcar. Requirements \u00b6 Python on Flatcar is automaticaly bootstrap with a portable Pypy . Sample configuration \u00b6 --- # bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): bootstrap_python : true ansible_python_interpreter : /opt/bin/python ansible_ssh_user : core ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /opt/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : | flex-volume-plugin-dir: \"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\" kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : | volumePluginDir: \"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\" kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd","title":"Flatcar"},{"location":"configuration/os/flatcar/#flatcar","text":"Here are sensible defaults variables to use when deploying on Flatcar.","title":"Flatcar"},{"location":"configuration/os/flatcar/#requirements","text":"Python on Flatcar is automaticaly bootstrap with a portable Pypy .","title":"Requirements"},{"location":"configuration/os/flatcar/#sample-configuration","text":"--- # bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): bootstrap_python : true ansible_python_interpreter : /opt/bin/python ansible_ssh_user : core ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /opt/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : | flex-volume-plugin-dir: \"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\" kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : | volumePluginDir: \"/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\" kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd","title":"Sample configuration"},{"location":"configuration/os/ubuntu/","text":"Ubuntu \u00b6 Here are sensible defaults variables to use when deploying on Ubuntu. Requirements \u00b6 To be able to use Ansible, at least python-minimal or python3-minimal must be installed. Sample configuration \u00b6 --- bootstrap_python : false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user : ubuntu ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : {} kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd","title":"Ubuntu"},{"location":"configuration/os/ubuntu/#ubuntu","text":"Here are sensible defaults variables to use when deploying on Ubuntu.","title":"Ubuntu"},{"location":"configuration/os/ubuntu/#requirements","text":"To be able to use Ansible, at least python-minimal or python3-minimal must be installed.","title":"Requirements"},{"location":"configuration/os/ubuntu/#sample-configuration","text":"--- bootstrap_python : false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user : ubuntu ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : {} kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd","title":"Sample configuration"},{"location":"user-guides/aws/","text":"Deploying on AWS \u00b6 Symplegma supports AWS as a cloud provider. Architecture \u00b6 contrib/aws/ contains terraform file to deploy the following architecture. This is strongly opinionated and deploys the following architecture: For now, each cluster has its own VPC, subnets, NAT etc. VPC module is included inside the symplegma module, this is subject to change in the future as PR are welcomed to make the possibilities evolved and split modules. Requirements \u00b6 Terraform Terragrunt Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml Terraform and Terragrunt \u00b6 Terragrunt is used to enable multiple cluster and environments, also to enable remote state storage and locking with Terraform. Terraform remote state is stored in an encrypted S3 bucket and state locking is done with AWS DynamoDB. Terragrunt modules \u00b6 Symplegma is packaged in a Terragrunt module available here . Terragrunt variables \u00b6 Remote state specific variables: terragrunt = { remote_state { backend = \"s3\" config { bucket = \"symplegma-remote-state\" key = \"${path_relative_to_include()}\" region = \"eu-west-1\" encrypt = true dynamodb_table = \"symplegma-remote-state-lock\" } } } Cluster specific variables: terragrunt = { include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/symplegma.git//contrib/aws/terraform/modules/symplegma\" } } // // [provider] // aws_region = \"eu-west-1\" // // [kubernetes] // cluster_name = \"symplegma\" // // [module][vpc] // vpc_name = \"symplegma\" vpc_cidr = \"10.0.0.0/16\" vpc_azs = [\"eu-west-1a\", \"eu-west-1b\", \"eu-west-1c\"] vpc_private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] vpc_public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] vpc_enable_nat_gateway = true vpc_enable_dns_hostnames = true vpc_tags = { Environment = \"sample\" } // // [module][bastion] // bastion_name = \"symplegma-bastion\" bastion_ami = \"ami-00035f41c82244dab\" bastion_instance_type = \"t2.small\" bastion_key_name = \"klefevre-sorrow\" bastion_tags = { Terraform = \"true\" Environment = \"sample\" } // // [module][master_asg] // master_asg_ami = \"ami-099b2d1bdd27b4649\" master_asg_root_volume_size = 50 master_asg_max_size = 3 master_asg_min_size = 3 master_asg_desired_capacity = 3 master_asg_instance_type = \"t3.large\" master_asg_key_name = \"klefevre-sorrow\" master_asg_tags = [ { key = \"Environment\" value = \"sample\" propagate_at_launch = true } ] // // [module][node_asg] // node_asg_ami = \"ami-099b2d1bdd27b4649\" node_asg_root_volume_size = 50 node_asg_max_size = 2 node_asg_min_size = 2 node_asg_desired_capacity = 2 node_asg_instance_type = \"t3.large\" node_asg_key_name = \"klefevre-sorrow\" node_asg_tags = [ { key = \"Environment\" value = \"sample\" propagate_at_launch = true } ] // // [nlb] // kubernetes_api_lb_port = 443 kubernetes_api_tg_port = 6443 kubernetes_api_lb_tags = { Environment = \"sample\" } Creating the infrastructure \u00b6 To init a new AWS cluster, simply run ./scripts/init-aws.sh $CLUSTER_NAME It will generate inventory/aws/$CLUSTER_NAME with the following directory structure: sample \u251c\u2500\u2500 aws.py -> ../../../contrib/aws/inventory/aws.py \u251c\u2500\u2500 extra_vars.yml \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 all \u2502 \u2514\u2500\u2500 all.yml \u251c\u2500\u2500 host_vars \u251c\u2500\u2500 symplegma-ansible.sh -> ../../../contrib/aws/scripts/symplegma-ansible.sh \u2514\u2500\u2500 tf_module_symplegma \u2514\u2500\u2500 terraform.tfvars Customizing the infrastructure \u00b6 Terraform variable files come with sensible default for the eu-west-1 region. If you wish to change remote state configuration you can edit $CLUSTER_NAME/terraform.tfvars If you wish to customize the infrastructure you can edit $CLUSTER_NAME/tf_module_symplegma/terraform.tfvars One of the most important variable is cluster_name that allows you tu use AWS dynamic inventory with multiple cluster. We recommend this variables to be coherent throughout your files and equals to $CLUSTER_NAME defined earlier. There is also a set of sensible default tags that you can customize such as Environment for example or add your own. To avoid bloating the configuration files and unnecessary hard coded values, Terraform provider credentials are derived from your AWS SDK config. Make sure you are using the correct aws profile by setting your AWS_PROFILE environment variable. Initializing the infrastructure \u00b6 Once everything is configured to your needs, just run: terragrunt apply-all --terragrunt-source-update Couples minute later you should see your instances spawning in your EC2 dashboard. Deploying Kubernetes with symplegma playbooks \u00b6 AWS Dynamic inventory \u00b6 AWS dynamic inventory allows you to target a specific set of instances depending on the $CLUSTER_NAME you set earlier. You can configure the behavior of dynamic inventory by setting the following ENV : export SYMPLEGMA_CLUSTER=$CLUSTER_NAME : Target only instances belonging to this cluster. export SYMPLEGMA_AWS_REGION=eu-west-1 : AWS region where instances are targeted. To test the behavior of the dynamic inventory just run: ./inventory/aws/ ${ CLUSTER_NAME } /aws.py --list It should only return a specific subset of your instances. Info These variables can be exported automatically when using the deployment script, but they can still be set manually for testing / manual deployment purposes. Customizing Kubernetes deployment \u00b6 In the cluster folder, it is possible to edit Ansible variables: group_vars/all/all.yml : contains default Ansible variables. --- bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user: ubuntu ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version: v1.24.1 kubernetes_version: v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir: /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes: {} kubeadm_scheduler_extra_args: {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config: {} kubeadm_kubelet_extra_args: {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config: {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans: {} kubeadm_cluster_name: symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime: containerd Info ansible_ssh_bastion_host , kubernetes_api_server_address and kubernetes_api_server_port can be automatically populated when using the deployment script but they can still be set manually for testing / manual deployment purposes. extra_vars : contains AWS cloud provider specific variables that you can override. --- kubeadm_api_server_extra_args: | cloud-provider: \"aws\" kubeadm_controller_manager_extra_args: | - cloud-provider: \"aws\" configure-cloud-routes: \"false\" kubeadm_scheduler_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: | cloud-provider: \"aws\" calico_mtu: 8981 calico_ipv4pool_ipip: \"CrossSubnet\" Info If you need to override control plane or kubelet specific parameters do it in extra_vars.yml as it overrides all other variables previously defined as per Ansible variables precedence documentantion Running the playbooks with deployment script \u00b6 A simple (really, it cannot be simpler) deployment script can call Ansible and compute the necessary Terraform output for you: #! /bin/sh INVENTORY_DIR = $( dirname \" ${ 0 } \" ) export SYMPLEGMA_CLUSTER = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all cluster_name 2 >/dev/null ) \" export SYMPLEGMA_AWS_REGION = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all aws_region 2 >/dev/null ) \" ansible-playbook -i \" ${ INVENTORY_DIR } \" /aws.py symplegma-init.yml -b -v \\ -e @ \" ${ INVENTORY_DIR } \" /extra_vars.yml \\ -e ansible_ssh_bastion_host = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all -module = bastion public_ip 2 >/dev/null ) \" \\ -e kubernetes_api_server_address = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all kubernetes_api_lb_dns_name 2 >/dev/null ) \" \\ -e kubernetes_api_server_port = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all kubernetes_api_lb_listener_port 2 >/dev/null ) \" \\ ${ @ } From the root of the repository just run your cluster deployment script: ./inventory/aws/ ${ CLUSTER_NAME } /symplegma-ansible.sh Testing cluster access \u00b6 When the deployment is over, admin.conf should be exported in kubeconfig/$CLUSTER_NAME/admin.conf . You should be able to call the Kubernetes API with kubectl : export KUBECONFIG = $( pwd ) /kubeconfig/ ${ CLUSTER_NAME } /admin.conf kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-1-140.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-1-22.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-2-47.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-2-8.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-3-123.eu-west-1.compute.internal Ready master 2d22h v1.13.0 Running E2E test with Sonobuoy \u00b6 If you want to test your cluster, you can use Sonobuoy which run the standard conformance testing suite on your cluster. Go to Sonobuoy scanner and just follow the instructions, the test results should be available after \u2154h. EOD","title":"AWS"},{"location":"user-guides/aws/#deploying-on-aws","text":"Symplegma supports AWS as a cloud provider.","title":"Deploying on AWS"},{"location":"user-guides/aws/#architecture","text":"contrib/aws/ contains terraform file to deploy the following architecture. This is strongly opinionated and deploys the following architecture: For now, each cluster has its own VPC, subnets, NAT etc. VPC module is included inside the symplegma module, this is subject to change in the future as PR are welcomed to make the possibilities evolved and split modules.","title":"Architecture"},{"location":"user-guides/aws/#requirements","text":"Terraform Terragrunt Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml","title":"Requirements"},{"location":"user-guides/aws/#terraform-and-terragrunt","text":"Terragrunt is used to enable multiple cluster and environments, also to enable remote state storage and locking with Terraform. Terraform remote state is stored in an encrypted S3 bucket and state locking is done with AWS DynamoDB.","title":"Terraform and Terragrunt"},{"location":"user-guides/aws/#terragrunt-modules","text":"Symplegma is packaged in a Terragrunt module available here .","title":"Terragrunt modules"},{"location":"user-guides/aws/#terragrunt-variables","text":"Remote state specific variables: terragrunt = { remote_state { backend = \"s3\" config { bucket = \"symplegma-remote-state\" key = \"${path_relative_to_include()}\" region = \"eu-west-1\" encrypt = true dynamodb_table = \"symplegma-remote-state-lock\" } } } Cluster specific variables: terragrunt = { include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/symplegma.git//contrib/aws/terraform/modules/symplegma\" } } // // [provider] // aws_region = \"eu-west-1\" // // [kubernetes] // cluster_name = \"symplegma\" // // [module][vpc] // vpc_name = \"symplegma\" vpc_cidr = \"10.0.0.0/16\" vpc_azs = [\"eu-west-1a\", \"eu-west-1b\", \"eu-west-1c\"] vpc_private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] vpc_public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] vpc_enable_nat_gateway = true vpc_enable_dns_hostnames = true vpc_tags = { Environment = \"sample\" } // // [module][bastion] // bastion_name = \"symplegma-bastion\" bastion_ami = \"ami-00035f41c82244dab\" bastion_instance_type = \"t2.small\" bastion_key_name = \"klefevre-sorrow\" bastion_tags = { Terraform = \"true\" Environment = \"sample\" } // // [module][master_asg] // master_asg_ami = \"ami-099b2d1bdd27b4649\" master_asg_root_volume_size = 50 master_asg_max_size = 3 master_asg_min_size = 3 master_asg_desired_capacity = 3 master_asg_instance_type = \"t3.large\" master_asg_key_name = \"klefevre-sorrow\" master_asg_tags = [ { key = \"Environment\" value = \"sample\" propagate_at_launch = true } ] // // [module][node_asg] // node_asg_ami = \"ami-099b2d1bdd27b4649\" node_asg_root_volume_size = 50 node_asg_max_size = 2 node_asg_min_size = 2 node_asg_desired_capacity = 2 node_asg_instance_type = \"t3.large\" node_asg_key_name = \"klefevre-sorrow\" node_asg_tags = [ { key = \"Environment\" value = \"sample\" propagate_at_launch = true } ] // // [nlb] // kubernetes_api_lb_port = 443 kubernetes_api_tg_port = 6443 kubernetes_api_lb_tags = { Environment = \"sample\" }","title":"Terragrunt variables"},{"location":"user-guides/aws/#creating-the-infrastructure","text":"To init a new AWS cluster, simply run ./scripts/init-aws.sh $CLUSTER_NAME It will generate inventory/aws/$CLUSTER_NAME with the following directory structure: sample \u251c\u2500\u2500 aws.py -> ../../../contrib/aws/inventory/aws.py \u251c\u2500\u2500 extra_vars.yml \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 all \u2502 \u2514\u2500\u2500 all.yml \u251c\u2500\u2500 host_vars \u251c\u2500\u2500 symplegma-ansible.sh -> ../../../contrib/aws/scripts/symplegma-ansible.sh \u2514\u2500\u2500 tf_module_symplegma \u2514\u2500\u2500 terraform.tfvars","title":"Creating the infrastructure"},{"location":"user-guides/aws/#customizing-the-infrastructure","text":"Terraform variable files come with sensible default for the eu-west-1 region. If you wish to change remote state configuration you can edit $CLUSTER_NAME/terraform.tfvars If you wish to customize the infrastructure you can edit $CLUSTER_NAME/tf_module_symplegma/terraform.tfvars One of the most important variable is cluster_name that allows you tu use AWS dynamic inventory with multiple cluster. We recommend this variables to be coherent throughout your files and equals to $CLUSTER_NAME defined earlier. There is also a set of sensible default tags that you can customize such as Environment for example or add your own. To avoid bloating the configuration files and unnecessary hard coded values, Terraform provider credentials are derived from your AWS SDK config. Make sure you are using the correct aws profile by setting your AWS_PROFILE environment variable.","title":"Customizing the infrastructure"},{"location":"user-guides/aws/#initializing-the-infrastructure","text":"Once everything is configured to your needs, just run: terragrunt apply-all --terragrunt-source-update Couples minute later you should see your instances spawning in your EC2 dashboard.","title":"Initializing the infrastructure"},{"location":"user-guides/aws/#deploying-kubernetes-with-symplegma-playbooks","text":"","title":"Deploying Kubernetes with symplegma playbooks"},{"location":"user-guides/aws/#aws-dynamic-inventory","text":"AWS dynamic inventory allows you to target a specific set of instances depending on the $CLUSTER_NAME you set earlier. You can configure the behavior of dynamic inventory by setting the following ENV : export SYMPLEGMA_CLUSTER=$CLUSTER_NAME : Target only instances belonging to this cluster. export SYMPLEGMA_AWS_REGION=eu-west-1 : AWS region where instances are targeted. To test the behavior of the dynamic inventory just run: ./inventory/aws/ ${ CLUSTER_NAME } /aws.py --list It should only return a specific subset of your instances. Info These variables can be exported automatically when using the deployment script, but they can still be set manually for testing / manual deployment purposes.","title":"AWS Dynamic inventory"},{"location":"user-guides/aws/#customizing-kubernetes-deployment","text":"In the cluster folder, it is possible to edit Ansible variables: group_vars/all/all.yml : contains default Ansible variables. --- bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user: ubuntu ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version: v1.24.1 kubernetes_version: v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir: /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes: {} kubeadm_scheduler_extra_args: {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config: {} kubeadm_kubelet_extra_args: {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config: {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans: {} kubeadm_cluster_name: symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime: containerd Info ansible_ssh_bastion_host , kubernetes_api_server_address and kubernetes_api_server_port can be automatically populated when using the deployment script but they can still be set manually for testing / manual deployment purposes. extra_vars : contains AWS cloud provider specific variables that you can override. --- kubeadm_api_server_extra_args: | cloud-provider: \"aws\" kubeadm_controller_manager_extra_args: | - cloud-provider: \"aws\" configure-cloud-routes: \"false\" kubeadm_scheduler_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: | cloud-provider: \"aws\" calico_mtu: 8981 calico_ipv4pool_ipip: \"CrossSubnet\" Info If you need to override control plane or kubelet specific parameters do it in extra_vars.yml as it overrides all other variables previously defined as per Ansible variables precedence documentantion","title":"Customizing Kubernetes deployment"},{"location":"user-guides/aws/#running-the-playbooks-with-deployment-script","text":"A simple (really, it cannot be simpler) deployment script can call Ansible and compute the necessary Terraform output for you: #! /bin/sh INVENTORY_DIR = $( dirname \" ${ 0 } \" ) export SYMPLEGMA_CLUSTER = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all cluster_name 2 >/dev/null ) \" export SYMPLEGMA_AWS_REGION = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all aws_region 2 >/dev/null ) \" ansible-playbook -i \" ${ INVENTORY_DIR } \" /aws.py symplegma-init.yml -b -v \\ -e @ \" ${ INVENTORY_DIR } \" /extra_vars.yml \\ -e ansible_ssh_bastion_host = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all -module = bastion public_ip 2 >/dev/null ) \" \\ -e kubernetes_api_server_address = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all kubernetes_api_lb_dns_name 2 >/dev/null ) \" \\ -e kubernetes_api_server_port = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all kubernetes_api_lb_listener_port 2 >/dev/null ) \" \\ ${ @ } From the root of the repository just run your cluster deployment script: ./inventory/aws/ ${ CLUSTER_NAME } /symplegma-ansible.sh","title":"Running the playbooks with deployment script"},{"location":"user-guides/aws/#testing-cluster-access","text":"When the deployment is over, admin.conf should be exported in kubeconfig/$CLUSTER_NAME/admin.conf . You should be able to call the Kubernetes API with kubectl : export KUBECONFIG = $( pwd ) /kubeconfig/ ${ CLUSTER_NAME } /admin.conf kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-1-140.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-1-22.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-2-47.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-2-8.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-3-123.eu-west-1.compute.internal Ready master 2d22h v1.13.0","title":"Testing cluster access"},{"location":"user-guides/aws/#running-e2e-test-with-sonobuoy","text":"If you want to test your cluster, you can use Sonobuoy which run the standard conformance testing suite on your cluster. Go to Sonobuoy scanner and just follow the instructions, the test results should be available after \u2154h. EOD","title":"Running E2E test with Sonobuoy"},{"location":"user-guides/bare-metal/","text":"Deploying on bare metal \u00b6 Symplegma supports deploying on bare metal. It is actually the main use case. symplegma-os_bootstrap supports bootstrapping python on Flatcar Linux but should also work with any OS manageable by Ansible as it installs binaries from sources and does not depends on distribution package manager. If you are using another distribution, just make sure python3-dev python3-pip or python-dev python-pip are installed and that the following kernel modules can be loaded. --- pip_python_coreos_modules : - httplib2 - six override_system_hostname : true bootstrap_python : false kernel_modules : - overlay - br_netfilter - ip_vs - ip_vs_rr - ip_vs_wrr - ip_vs_sh kernel_parameters : - net.bridge.bridge-nf-call-iptables - net.bridge.bridge-nf-call-arptables - net.bridge.bridge-nf-call-ip6tables - net.ipv4.ip_forward bin_dir : /opt/bin sysctl_file_path : \"/etc/sysctl.d/99-kubernetes.conf\" module_load_path : \"/etc/modules-load.d/99-kubernetes.conf\" etcd_version : 3.5.4 etcd_release_url : https://github.com/etcd-io/etcd/releases/download/v{{ etcd_version }}/etcd-v{{ etcd_version }}-linux-amd64.tar.gz etcd_release_dir : /opt/etcd jq_version : 1.6 jq_release_url : https://github.com/stedolan/jq/releases/download/jq-{{ jq_version }}/jq-linux64 You can contribute to the bootstrap role here Requirements \u00b6 Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml Preparing inventory \u00b6 Simply copy the sample inventory to another folder with the desired cluster name: cp -ar inventory/ubuntu inventory/$CLUSTER_NAME Create an inventory in a compatible format , for example in inventory/$CLUSTER_NAME/hosts file: k8s-master-1.clusterfrak-dynamics.io k8s-worker-1.clusterfrak-dynamics.io k8s-worker-2.clusterfrak-dynamics.io k8s-worker-3.clusterfrak-dynamics.io k8s-worker-4.clusterfrak-dynamics.io [master] k8s-master-1.clusterfrak-dynamics.io [node] k8s-worker-1.clusterfrak-dynamics.io k8s-worker-2.clusterfrak-dynamics.io k8s-worker-3.clusterfrak-dynamics.io k8s-worker-4.clusterfrak-dynamics.io Your directory structure should be the following: tree -I 'roles|contrib|docs|scripts|sample' . \u251c\u2500\u2500 README.md \u2514\u2500\u2500 symplegma \u251c\u2500\u2500 CODEOWNERS \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 inventory \u2502 \u2514\u2500\u2500 $CLUSTER_NAME \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 all \u2502 \u2502 \u2514\u2500\u2500 all.yml \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 hosts \u251c\u2500\u2500 kubeconfig \u2502 \u2514\u2500\u2500 $CLUSTER_NAME \u2502 \u2514\u2500\u2500 admin.conf \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 requirements.yml \u251c\u2500\u2500 symplegma-init.yml \u251c\u2500\u2500 symplegma-reset.yml \u2514\u2500\u2500 symplegma-upgrade.yml Configuring the cluster \u00b6 Cluster configuration is done in inventory/$CLUSTER_NAME/group_vars/all/all.yml : --- bootstrap_python : false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user : ubuntu ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : {} kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd Some lines have been modified compared to the original file assumes you are running behind a bastion host. Here we connect directly to the host through SSH. kubeadm version and kubernetes version can also be customized here, please note by default kubernetes_version is equal to kubeadm_version but not the opposite : --- bin_dir : /opt/bin cri_socket : unix:///run/containerd/containerd.sock kubeadm_version : v1.24.1 kubernetes_version : \"{{ kubeadm_version }}\" kubernetes_api_endpoint_port : 6443 kubernetes_binaries_url : https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/amd64 kubeadm_exec_dir : /root kubeadm_sync_dirs : - /etc/kubernetes - /etc/kubernetes/pki - /etc/kubernetes/pki/etcd kubeadm_pod_subnet : 192.168.0.0/16 kubeadm_service_subnet : 10.96.0.0/12 kubeadm_service_dns_domain : cluster.local kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} kubeadm_api_server_cert_extra_sans : {} kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} kubeadm_scheduler_extra_args : {} kubeadm_scheduler_extra_volumes : {} kubeadm_kubelet_extra_args : {} kubeadm_kubelet_component_config : {} kubeadm_kube_proxy_component_config : {} kubeadm_cluster_name : symplegma kubeadm_force_init : false kubeadm_mark_control_plane : true systemd_cgroup : false This allow to create custom deployments and custom upgrade logic (updating Kubelet before control plane for example). Running the playbooks \u00b6 Playbooks can be run from the symplegma directory of the repository: sudo ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-init.yml -v The following tags are also availabled to run specific roles: bootstrap : bootstrap OS for ansible. containerd : install Kubernetes default container runtime. cni : install container network interface plugins. kubernetes_hosts : install Kubernetes binaries and Kubeadm. kubeadm-master : bootstrap Kubeadm master nodes kuebadm-nodes : bootstrap Kubeadm worker nodes Upgrading the cluster \u00b6 There is another playbook symplegma-upgrade.yml which does the same thing as symplegma-init.yml but with a serial set to 1. It means that nodes will be upgraded one by one. To update Kubernetes to another version, just change kubeadm_version and kubernetes_version in symplegma/inventory/$CLUSTER_NAME/group_vars/all/all.yml and re-run the playbooks for kubernetes_host and kubeadm-master . ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-upgrade.yml -v --tags kubernetes_hosts ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-upgrade.yml -v --tags kubeadm-master Accessing the cluster \u00b6 When the playbbok are done, a kubeconfig file is generated in symplegma/kubeconfig/$CLUSTER_NAME/admin.conf . This file contains the TLS certificates to access the cluster as an administrator. By default, kubectl looks into ~/.kube/config . To use the generated kubeconfig file from the symplegma directory: export KUBECONFIG=$(pwd)/kubeconfig/$CLUSTER_NAME/admin.conf Check that you can access the cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-1 Ready master 25h v1.15.0 k8s-worker-1 Ready <none> 25h v1.15.0 k8s-worker-2 Ready <none> 25h v1.15.0 k8s-worker-3 Ready <none> 25h v1.15.0 k8s-worker-4 Ready <none> 25h v1.15.0 kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6fb584dd97-wfj5r 1/1 Running 1 7d20h kube-system calico-node-554qz 1/1 Running 1 7d20h kube-system calico-node-c28zf 1/1 Running 1 7d20h kube-system calico-node-glqjd 1/1 Running 1 7d20h kube-system calico-node-lm9hp 1/1 Running 0 20h kube-system calico-node-qkw4j 1/1 Running 1 7d20h kube-system coredns-5c98db65d4-kvsnz 1/1 Running 2 7d20h kube-system coredns-5c98db65d4-s8zsb 1/1 Running 2 7d20h kube-system etcd-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-apiserver-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-controller-manager-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-proxy-dxllt 1/1 Running 1 7d20h kube-system kube-proxy-h97vq 1/1 Running 0 20h kube-system kube-proxy-kt9gc 1/1 Running 1 7d20h kube-system kube-proxy-ngw7p 1/1 Running 1 7d20h kube-system kube-proxy-rgh7t 1/1 Running 1 7d20h kube-system kube-scheduler-k8s-master-1-dev 1/1 Running 2 7d20h kube-system nginx-proxy-k8s-worker-1-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-2-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-3-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-4-dev 1/1 Running 1 20h Conformance end to end test \u00b6 The cluster has passed Kubernetes conformance testing. These tests are run with sonobuoy . Install Sonobuoy \u00b6 Install sonobuoy : by downloading the binary -> go to https://github.com/heptio/sonobuoy/releases alternatively, on a machine with Go installed: go get -u -v github.com/heptio/sonobuoy Run sonobuoy manually \u00b6 To run sonobuoy (the right kubeconfig must be set as sonobuoy talks to the cluster): sonobuoy run The tests might take between 60 minutes and 2h. To check the status: sonobuoy status To retrieve the results once it is done: sonobuoy retrieve To inspect results: sonobuoy e2e 201906261404_sonobuoy_4aff5e8e-21a8-448c-8960-c6565b92be91.tar.gz failed tests: 0 Then delete cluster resources: sonobuoy delete","title":"Bare-Metal"},{"location":"user-guides/bare-metal/#deploying-on-bare-metal","text":"Symplegma supports deploying on bare metal. It is actually the main use case. symplegma-os_bootstrap supports bootstrapping python on Flatcar Linux but should also work with any OS manageable by Ansible as it installs binaries from sources and does not depends on distribution package manager. If you are using another distribution, just make sure python3-dev python3-pip or python-dev python-pip are installed and that the following kernel modules can be loaded. --- pip_python_coreos_modules : - httplib2 - six override_system_hostname : true bootstrap_python : false kernel_modules : - overlay - br_netfilter - ip_vs - ip_vs_rr - ip_vs_wrr - ip_vs_sh kernel_parameters : - net.bridge.bridge-nf-call-iptables - net.bridge.bridge-nf-call-arptables - net.bridge.bridge-nf-call-ip6tables - net.ipv4.ip_forward bin_dir : /opt/bin sysctl_file_path : \"/etc/sysctl.d/99-kubernetes.conf\" module_load_path : \"/etc/modules-load.d/99-kubernetes.conf\" etcd_version : 3.5.4 etcd_release_url : https://github.com/etcd-io/etcd/releases/download/v{{ etcd_version }}/etcd-v{{ etcd_version }}-linux-amd64.tar.gz etcd_release_dir : /opt/etcd jq_version : 1.6 jq_release_url : https://github.com/stedolan/jq/releases/download/jq-{{ jq_version }}/jq-linux64 You can contribute to the bootstrap role here","title":"Deploying on bare metal"},{"location":"user-guides/bare-metal/#requirements","text":"Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml","title":"Requirements"},{"location":"user-guides/bare-metal/#preparing-inventory","text":"Simply copy the sample inventory to another folder with the desired cluster name: cp -ar inventory/ubuntu inventory/$CLUSTER_NAME Create an inventory in a compatible format , for example in inventory/$CLUSTER_NAME/hosts file: k8s-master-1.clusterfrak-dynamics.io k8s-worker-1.clusterfrak-dynamics.io k8s-worker-2.clusterfrak-dynamics.io k8s-worker-3.clusterfrak-dynamics.io k8s-worker-4.clusterfrak-dynamics.io [master] k8s-master-1.clusterfrak-dynamics.io [node] k8s-worker-1.clusterfrak-dynamics.io k8s-worker-2.clusterfrak-dynamics.io k8s-worker-3.clusterfrak-dynamics.io k8s-worker-4.clusterfrak-dynamics.io Your directory structure should be the following: tree -I 'roles|contrib|docs|scripts|sample' . \u251c\u2500\u2500 README.md \u2514\u2500\u2500 symplegma \u251c\u2500\u2500 CODEOWNERS \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 inventory \u2502 \u2514\u2500\u2500 $CLUSTER_NAME \u2502 \u251c\u2500\u2500 group_vars \u2502 \u2502 \u2514\u2500\u2500 all \u2502 \u2502 \u2514\u2500\u2500 all.yml \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 hosts \u251c\u2500\u2500 kubeconfig \u2502 \u2514\u2500\u2500 $CLUSTER_NAME \u2502 \u2514\u2500\u2500 admin.conf \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 requirements.yml \u251c\u2500\u2500 symplegma-init.yml \u251c\u2500\u2500 symplegma-reset.yml \u2514\u2500\u2500 symplegma-upgrade.yml","title":"Preparing inventory"},{"location":"user-guides/bare-metal/#configuring-the-cluster","text":"Cluster configuration is done in inventory/$CLUSTER_NAME/group_vars/all/all.yml : --- bootstrap_python : false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user : ubuntu ansible_ssh_common_args : '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version : v1.24.1 kubernetes_version : v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir : /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes : {} kubeadm_scheduler_extra_args : {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config : {} kubeadm_kubelet_extra_args : {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config : {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans : {} kubeadm_cluster_name : symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime : containerd Some lines have been modified compared to the original file assumes you are running behind a bastion host. Here we connect directly to the host through SSH. kubeadm version and kubernetes version can also be customized here, please note by default kubernetes_version is equal to kubeadm_version but not the opposite : --- bin_dir : /opt/bin cri_socket : unix:///run/containerd/containerd.sock kubeadm_version : v1.24.1 kubernetes_version : \"{{ kubeadm_version }}\" kubernetes_api_endpoint_port : 6443 kubernetes_binaries_url : https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/amd64 kubeadm_exec_dir : /root kubeadm_sync_dirs : - /etc/kubernetes - /etc/kubernetes/pki - /etc/kubernetes/pki/etcd kubeadm_pod_subnet : 192.168.0.0/16 kubeadm_service_subnet : 10.96.0.0/12 kubeadm_service_dns_domain : cluster.local kubeadm_api_server_extra_args : {} kubeadm_api_server_extra_volumes : {} kubeadm_api_server_cert_extra_sans : {} kubeadm_controller_manager_extra_args : {} kubeadm_controller_manager_extra_volumes : {} kubeadm_scheduler_extra_args : {} kubeadm_scheduler_extra_volumes : {} kubeadm_kubelet_extra_args : {} kubeadm_kubelet_component_config : {} kubeadm_kube_proxy_component_config : {} kubeadm_cluster_name : symplegma kubeadm_force_init : false kubeadm_mark_control_plane : true systemd_cgroup : false This allow to create custom deployments and custom upgrade logic (updating Kubelet before control plane for example).","title":"Configuring the cluster"},{"location":"user-guides/bare-metal/#running-the-playbooks","text":"Playbooks can be run from the symplegma directory of the repository: sudo ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-init.yml -v The following tags are also availabled to run specific roles: bootstrap : bootstrap OS for ansible. containerd : install Kubernetes default container runtime. cni : install container network interface plugins. kubernetes_hosts : install Kubernetes binaries and Kubeadm. kubeadm-master : bootstrap Kubeadm master nodes kuebadm-nodes : bootstrap Kubeadm worker nodes","title":"Running the playbooks"},{"location":"user-guides/bare-metal/#upgrading-the-cluster","text":"There is another playbook symplegma-upgrade.yml which does the same thing as symplegma-init.yml but with a serial set to 1. It means that nodes will be upgraded one by one. To update Kubernetes to another version, just change kubeadm_version and kubernetes_version in symplegma/inventory/$CLUSTER_NAME/group_vars/all/all.yml and re-run the playbooks for kubernetes_host and kubeadm-master . ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-upgrade.yml -v --tags kubernetes_hosts ansible-playbook -i inventory/$CLUSTER_NAME/hosts -b symplegma-upgrade.yml -v --tags kubeadm-master","title":"Upgrading the cluster"},{"location":"user-guides/bare-metal/#accessing-the-cluster","text":"When the playbbok are done, a kubeconfig file is generated in symplegma/kubeconfig/$CLUSTER_NAME/admin.conf . This file contains the TLS certificates to access the cluster as an administrator. By default, kubectl looks into ~/.kube/config . To use the generated kubeconfig file from the symplegma directory: export KUBECONFIG=$(pwd)/kubeconfig/$CLUSTER_NAME/admin.conf Check that you can access the cluster: kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-1 Ready master 25h v1.15.0 k8s-worker-1 Ready <none> 25h v1.15.0 k8s-worker-2 Ready <none> 25h v1.15.0 k8s-worker-3 Ready <none> 25h v1.15.0 k8s-worker-4 Ready <none> 25h v1.15.0 kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6fb584dd97-wfj5r 1/1 Running 1 7d20h kube-system calico-node-554qz 1/1 Running 1 7d20h kube-system calico-node-c28zf 1/1 Running 1 7d20h kube-system calico-node-glqjd 1/1 Running 1 7d20h kube-system calico-node-lm9hp 1/1 Running 0 20h kube-system calico-node-qkw4j 1/1 Running 1 7d20h kube-system coredns-5c98db65d4-kvsnz 1/1 Running 2 7d20h kube-system coredns-5c98db65d4-s8zsb 1/1 Running 2 7d20h kube-system etcd-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-apiserver-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-controller-manager-k8s-master-1-dev 1/1 Running 2 7d20h kube-system kube-proxy-dxllt 1/1 Running 1 7d20h kube-system kube-proxy-h97vq 1/1 Running 0 20h kube-system kube-proxy-kt9gc 1/1 Running 1 7d20h kube-system kube-proxy-ngw7p 1/1 Running 1 7d20h kube-system kube-proxy-rgh7t 1/1 Running 1 7d20h kube-system kube-scheduler-k8s-master-1-dev 1/1 Running 2 7d20h kube-system nginx-proxy-k8s-worker-1-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-2-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-3-dev 1/1 Running 1 7d20h kube-system nginx-proxy-k8s-worker-4-dev 1/1 Running 1 20h","title":"Accessing the cluster"},{"location":"user-guides/bare-metal/#conformance-end-to-end-test","text":"The cluster has passed Kubernetes conformance testing. These tests are run with sonobuoy .","title":"Conformance end to end test"},{"location":"user-guides/bare-metal/#install-sonobuoy","text":"Install sonobuoy : by downloading the binary -> go to https://github.com/heptio/sonobuoy/releases alternatively, on a machine with Go installed: go get -u -v github.com/heptio/sonobuoy","title":"Install Sonobuoy"},{"location":"user-guides/bare-metal/#run-sonobuoy-manually","text":"To run sonobuoy (the right kubeconfig must be set as sonobuoy talks to the cluster): sonobuoy run The tests might take between 60 minutes and 2h. To check the status: sonobuoy status To retrieve the results once it is done: sonobuoy retrieve To inspect results: sonobuoy e2e 201906261404_sonobuoy_4aff5e8e-21a8-448c-8960-c6565b92be91.tar.gz failed tests: 0 Then delete cluster resources: sonobuoy delete","title":"Run sonobuoy manually"},{"location":"user-guides/openstack/","text":"Deploying on OpenStack \u00b6 Symplegma supports OpenStack as a cloud provider. Architecture \u00b6 For now, each cluster should have its own OpenStack project. Modules used are from the Kubepsray project and are included inside the symplegma module, this is subject to change in the future as PR are welcomed to make the possibilities evolved and split modules. Requirements \u00b6 Terraform Terragrunt Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml Terraform and Terragrunt \u00b6 Terragrunt is used to enable multiple cluster and environments. Terragrunt modules \u00b6 Symplegma is packaged in a Terragrunt module available here . Terragrunt variables \u00b6 Cluster specific variables: terragrunt = { include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/symplegma.git//contrib/openstack/terraform/modules/symplegma\" } } // // [provider] // // // [kubernetes] // cluster_name = \"symplegma\" network_name = \"internal_network\" subnet_cidr = \"10.0.0.0/24\" dns_nameservers = [] use_neutron = \"1\" number_of_k8s_masters = \"3\" number_of_k8s_masters_no_etcd = \"0\" number_of_k8s_nodes = \"0\" floatingip_pool = \"ext-net\" number_of_bastions = \"0\" external_net = \"ext-net-uuid\" router_id = \"\" az_list = [\"nova\"] number_of_etcd = \"0\" number_of_k8s_masters_no_floating_ip = \"0\" number_of_k8s_masters_no_floating_ip_no_etcd = \"0\" number_of_k8s_nodes_no_floating_ip = \"0\" public_key_path = \"~/.ssh/id_rsa.pub\" image = \"CoreOS 1068.9.0\" ssh_user = \"core\" flavor_k8s_master = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_k8s_node = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_etcd = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_bastion = \"128829e3-117d-49da-ae58-981bb2c04b0e\" k8s_master_fips = [] k8s_node_fips = [] bastion_fips = [] bastion_allowed_remote_ips = [\"0.0.0.0/0\"] supplementary_master_groups = \"\" supplementary_node_groups = \"\" worker_allowed_ports = [ { \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ] Creating the infrastructure \u00b6 To init a new OpenStack cluster, simply run ./scripts/init-openstack.sh $CLUSTER_NAME It will generate inventory/openstack/$CLUSTER_NAME with the following directory structure: sample \u251c\u2500\u2500 openstack.py -> ../../../contrib/openstack/inventory/openstack.py \u251c\u2500\u2500 extra_vars.yml \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 all \u2502 \u2514\u2500\u2500 all.yml \u251c\u2500\u2500 host_vars \u251c\u2500\u2500 symplegma-ansible.sh -> ../../../contrib/openstack/scripts/symplegma-ansible.sh \u2514\u2500\u2500 tf_module_symplegma \u2514\u2500\u2500 terraform.tfvars Customizing the infrastructure \u00b6 Terraform variable files come with sensible default. If you wish to change remote state configuration you can edit $CLUSTER_NAME/terraform.tfvars If you wish to customize the infrastructure you can edit $CLUSTER_NAME/tf_module_symplegma/terraform.tfvars One of the most important variable is cluster_name that allows you tu use OpenStack dynamic inventory with multiple cluster. We recommend this variables to be coherent throughout your files and equals to $CLUSTER_NAME defined earlier. There is also a set of sensible default tags that you can customize such as Environment for example or add your own. To avoid bloating the configuration files and unnecessary hard coded values, Terraform provider credentials are derived from your OpenStack SDK config. Make sure you are using the correct OpenStack credentials by setting your OS_CLOUD environment variable. [more infos] Initializing the infrastructure \u00b6 Once everything is configured to your needs, just run: terragrunt apply-all --terragrunt-source-update Couples minute later you should see your instances spawning in your Horizon dashboard. Deploying Kubernetes with symplegma playbooks \u00b6 OpenStack Dynamic inventory \u00b6 OpenStack dynamic inventory allows you to target a specific set of instances depending on the $CLUSTER_NAME you set earlier. You can configure the behavior of dynamic inventory by setting the following ENV : export SYMPLEGMA_CLUSTER=$CLUSTER_NAME : Target only instances belonging to this cluster. To test the behavior of the dynamic inventory just run: ./inventory/openstack/ ${ CLUSTER_NAME } /openstack.py --list It should only return a specific subset of your instances. Info These variables can be exported automatically when using the deployment script, but they can still be set manually for testing / manual deployment purposes. Customizing Kubernetes deployment \u00b6 In the cluster folder, it is possible to edit Ansible variables: group_vars/all/all.yml : contains default Ansible variables. --- bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user: ubuntu ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version: v1.24.1 kubernetes_version: v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir: /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes: {} kubeadm_scheduler_extra_args: {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config: {} kubeadm_kubelet_extra_args: {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config: {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans: {} kubeadm_cluster_name: symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime: containerd Info ansible_ssh_bastion_host , kubernetes_api_server_address and kubernetes_api_server_port can be automatically populated when using the deployment script but they can still be set manually for testing / manual deployment purposes. extra_vars : contains OpenStack cloud provider specific variables that you can override. --- kubeadm_api_server_extra_args: | cloud-provider: \"openstack\" kubeadm_controller_manager_extra_args: | - cloud-provider: \"openstack\" configure-cloud-routes: \"false\" kubeadm_scheduler_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: | cloud-provider: \"openstack\" calico_mtu: 1430 calico_ipv4pool_ipip: \"Always\" calico_felix_ipip: \"true\" Info If you need to override control plane or kubelet specific parameters do it in extra_vars.yml as it overrides all other variables previously defined as per Ansible variables precedence documentantion Running the playbooks with deployment script \u00b6 A simple (really, it cannot be simpler) deployment script can call Ansible and compute the necessary Terraform output for you: #! /bin/sh INVENTORY_DIR = $( dirname \" ${ 0 } \" ) export SYMPLEGMA_CLUSTER = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all cluster_name 2 >/dev/null ) \" ansible-playbook -i \" ${ INVENTORY_DIR } \" /hosts.ini symplegma-init.yml -b -v \\ -e @ \" ${ INVENTORY_DIR } \" /extra_vars.yml \\ -e ansible_ssh_bastion_host = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all bastion_fips 2 >/dev/null ) \" \\ \" $@ \" From the root of the repository just run your cluster deployment script: ./inventory/openstack/ ${ CLUSTER_NAME } /symplegma-ansible.sh Testing cluster access \u00b6 When the deployment is over, admin.conf should be exported in kubeconfig/$CLUSTER_NAME/admin.conf . You should be able to call the Kubernetes API with kubectl : export KUBECONFIG = $( pwd ) /kubeconfig/ ${ CLUSTER_NAME } /admin.conf kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-1-140.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-1-22.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-2-47.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-2-8.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-3-123.eu-west-1.compute.internal Ready master 2d22h v1.13.0 EOD","title":"OpenStack"},{"location":"user-guides/openstack/#deploying-on-openstack","text":"Symplegma supports OpenStack as a cloud provider.","title":"Deploying on OpenStack"},{"location":"user-guides/openstack/#architecture","text":"For now, each cluster should have its own OpenStack project. Modules used are from the Kubepsray project and are included inside the symplegma module, this is subject to change in the future as PR are welcomed to make the possibilities evolved and split modules.","title":"Architecture"},{"location":"user-guides/openstack/#requirements","text":"Terraform Terragrunt Ansible kubectl Git clone Symplegma main repository: git clone https://github.com/clusterfrak-dynamics/symplegma.git Fetch the roles with ansible-galaxy : ansible-galaxy install -r requirements.yml","title":"Requirements"},{"location":"user-guides/openstack/#terraform-and-terragrunt","text":"Terragrunt is used to enable multiple cluster and environments.","title":"Terraform and Terragrunt"},{"location":"user-guides/openstack/#terragrunt-modules","text":"Symplegma is packaged in a Terragrunt module available here .","title":"Terragrunt modules"},{"location":"user-guides/openstack/#terragrunt-variables","text":"Cluster specific variables: terragrunt = { include { path = \"${find_in_parent_folders()}\" } terraform { source = \"github.com/clusterfrak-dynamics/symplegma.git//contrib/openstack/terraform/modules/symplegma\" } } // // [provider] // // // [kubernetes] // cluster_name = \"symplegma\" network_name = \"internal_network\" subnet_cidr = \"10.0.0.0/24\" dns_nameservers = [] use_neutron = \"1\" number_of_k8s_masters = \"3\" number_of_k8s_masters_no_etcd = \"0\" number_of_k8s_nodes = \"0\" floatingip_pool = \"ext-net\" number_of_bastions = \"0\" external_net = \"ext-net-uuid\" router_id = \"\" az_list = [\"nova\"] number_of_etcd = \"0\" number_of_k8s_masters_no_floating_ip = \"0\" number_of_k8s_masters_no_floating_ip_no_etcd = \"0\" number_of_k8s_nodes_no_floating_ip = \"0\" public_key_path = \"~/.ssh/id_rsa.pub\" image = \"CoreOS 1068.9.0\" ssh_user = \"core\" flavor_k8s_master = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_k8s_node = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_etcd = \"128829e3-117d-49da-ae58-981bb2c04b0e\" flavor_bastion = \"128829e3-117d-49da-ae58-981bb2c04b0e\" k8s_master_fips = [] k8s_node_fips = [] bastion_fips = [] bastion_allowed_remote_ips = [\"0.0.0.0/0\"] supplementary_master_groups = \"\" supplementary_node_groups = \"\" worker_allowed_ports = [ { \"protocol\" = \"tcp\" \"port_range_min\" = 30000 \"port_range_max\" = 32767 \"remote_ip_prefix\" = \"0.0.0.0/0\" } ]","title":"Terragrunt variables"},{"location":"user-guides/openstack/#creating-the-infrastructure","text":"To init a new OpenStack cluster, simply run ./scripts/init-openstack.sh $CLUSTER_NAME It will generate inventory/openstack/$CLUSTER_NAME with the following directory structure: sample \u251c\u2500\u2500 openstack.py -> ../../../contrib/openstack/inventory/openstack.py \u251c\u2500\u2500 extra_vars.yml \u251c\u2500\u2500 group_vars \u2502 \u2514\u2500\u2500 all \u2502 \u2514\u2500\u2500 all.yml \u251c\u2500\u2500 host_vars \u251c\u2500\u2500 symplegma-ansible.sh -> ../../../contrib/openstack/scripts/symplegma-ansible.sh \u2514\u2500\u2500 tf_module_symplegma \u2514\u2500\u2500 terraform.tfvars","title":"Creating the infrastructure"},{"location":"user-guides/openstack/#customizing-the-infrastructure","text":"Terraform variable files come with sensible default. If you wish to change remote state configuration you can edit $CLUSTER_NAME/terraform.tfvars If you wish to customize the infrastructure you can edit $CLUSTER_NAME/tf_module_symplegma/terraform.tfvars One of the most important variable is cluster_name that allows you tu use OpenStack dynamic inventory with multiple cluster. We recommend this variables to be coherent throughout your files and equals to $CLUSTER_NAME defined earlier. There is also a set of sensible default tags that you can customize such as Environment for example or add your own. To avoid bloating the configuration files and unnecessary hard coded values, Terraform provider credentials are derived from your OpenStack SDK config. Make sure you are using the correct OpenStack credentials by setting your OS_CLOUD environment variable. [more infos]","title":"Customizing the infrastructure"},{"location":"user-guides/openstack/#initializing-the-infrastructure","text":"Once everything is configured to your needs, just run: terragrunt apply-all --terragrunt-source-update Couples minute later you should see your instances spawning in your Horizon dashboard.","title":"Initializing the infrastructure"},{"location":"user-guides/openstack/#deploying-kubernetes-with-symplegma-playbooks","text":"","title":"Deploying Kubernetes with symplegma playbooks"},{"location":"user-guides/openstack/#openstack-dynamic-inventory","text":"OpenStack dynamic inventory allows you to target a specific set of instances depending on the $CLUSTER_NAME you set earlier. You can configure the behavior of dynamic inventory by setting the following ENV : export SYMPLEGMA_CLUSTER=$CLUSTER_NAME : Target only instances belonging to this cluster. To test the behavior of the dynamic inventory just run: ./inventory/openstack/ ${ CLUSTER_NAME } /openstack.py --list It should only return a specific subset of your instances. Info These variables can be exported automatically when using the deployment script, but they can still be set manually for testing / manual deployment purposes.","title":"OpenStack Dynamic inventory"},{"location":"user-guides/openstack/#customizing-kubernetes-deployment","text":"In the cluster folder, it is possible to edit Ansible variables: group_vars/all/all.yml : contains default Ansible variables. --- bootstrap_python: false # Install portable python distribution that do not provide python (eg. # coreos/flatcar): # bootstrap_python: true # ansible_python_interpreter: /opt/bin/python ansible_ssh_user: ubuntu ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # To use a bastion host between node and ansible use: # ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -W %h:%p -q ubuntu@{{ ansible_ssh_bastion_host }}\"' # ansible_ssh_bastion_host: __BASTION_IP__ kubeadm_version: v1.24.1 kubernetes_version: v1.24.1 # If deploying HA clusters, specify the loadbalancer IP or domain name and port # in front of the control plane nodes: # kubernetes_api_server_address: __LB_HOSTNAME__ # kubernetes_api_server_port: __LB_LISTENER_PORT__ bin_dir: /usr/local/bin # Change default path for custom binary. On OS with immutable file system (eg. # coreos/flatcar) use a writable path # bin_dir: /opt/bin # Customize API server kubeadm_api_server_extra_args: {} kubeadm_api_server_extra_volumes: {} # Customize controller manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_controller_manager_extra_args: | # address: 0.0.0.0 kubeadm_controller_manager_extra_args: {} kubeadm_controller_manager_extra_volumes: {} # Customize scheduler manager scheduler # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_scheduler_extra_args: | # address: 0.0.0.0 kubeadm_scheduler_extra_volumes: {} kubeadm_scheduler_extra_args: {} # Customize Kubelet # `kubeadm_kubelet_extra_args` is to be used as a last resort, # `kubeadm_kubelet_component_config` configure kubelet wth native kubeadm API, # please see # https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for # more information kubeadm_kubelet_component_config: {} kubeadm_kubelet_extra_args: {} # Customize Kube Proxy configuration using native Kubeadm API # eg. to publish prometheus metrics on \"0.0.0.0\": # kubeadm_kube_proxy_component_config: | # metricsBindAddress: 0.0.0.0 kubeadm_kube_proxy_component_config: {} # Additionnal subject alternative names for the API server # eg. to add aditionnals domains: # kubeadm_api_server_cert_extra_sans: | # - mydomain.example.com kubeadm_api_server_cert_extra_sans: {} kubeadm_cluster_name: symplegma # Do not label master nor taint (skip kubeadm phase) # kubeadm_mark_control_plane: false # Enable systemd cgroup for Kubelet and container runtime # DO NOT CHANGE this on an existing cluster: Changing the cgroup driver of a # Node that has joined a cluster is strongly not recommended. If the kubelet # has created Pods using the semantics of one cgroup driver, changing the # container runtime to another cgroup driver can cause errors when trying to # re-create the Pod sandbox for such existing Pods. Restarting the kubelet may # not solve such errors. Default is to use cgroupfs. # systemd_cgroup: true container_runtime: containerd Info ansible_ssh_bastion_host , kubernetes_api_server_address and kubernetes_api_server_port can be automatically populated when using the deployment script but they can still be set manually for testing / manual deployment purposes. extra_vars : contains OpenStack cloud provider specific variables that you can override. --- kubeadm_api_server_extra_args: | cloud-provider: \"openstack\" kubeadm_controller_manager_extra_args: | - cloud-provider: \"openstack\" configure-cloud-routes: \"false\" kubeadm_scheduler_extra_args: {} kubeadm_api_server_extra_volumes: {} kubeadm_controller_manager_extra_volumes: {} kubeadm_scheduler_extra_volumes: {} kubeadm_kubelet_extra_args: | cloud-provider: \"openstack\" calico_mtu: 1430 calico_ipv4pool_ipip: \"Always\" calico_felix_ipip: \"true\" Info If you need to override control plane or kubelet specific parameters do it in extra_vars.yml as it overrides all other variables previously defined as per Ansible variables precedence documentantion","title":"Customizing Kubernetes deployment"},{"location":"user-guides/openstack/#running-the-playbooks-with-deployment-script","text":"A simple (really, it cannot be simpler) deployment script can call Ansible and compute the necessary Terraform output for you: #! /bin/sh INVENTORY_DIR = $( dirname \" ${ 0 } \" ) export SYMPLEGMA_CLUSTER = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all cluster_name 2 >/dev/null ) \" ansible-playbook -i \" ${ INVENTORY_DIR } \" /hosts.ini symplegma-init.yml -b -v \\ -e @ \" ${ INVENTORY_DIR } \" /extra_vars.yml \\ -e ansible_ssh_bastion_host = \" $( cd \" ${ INVENTORY_DIR } \" && terragrunt output-all bastion_fips 2 >/dev/null ) \" \\ \" $@ \" From the root of the repository just run your cluster deployment script: ./inventory/openstack/ ${ CLUSTER_NAME } /symplegma-ansible.sh","title":"Running the playbooks with deployment script"},{"location":"user-guides/openstack/#testing-cluster-access","text":"When the deployment is over, admin.conf should be exported in kubeconfig/$CLUSTER_NAME/admin.conf . You should be able to call the Kubernetes API with kubectl : export KUBECONFIG = $( pwd ) /kubeconfig/ ${ CLUSTER_NAME } /admin.conf kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-1-140.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-1-22.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-2-47.eu-west-1.compute.internal Ready <none> 2d22h v1.13.0 ip-10-0-2-8.eu-west-1.compute.internal Ready master 2d22h v1.13.0 ip-10-0-3-123.eu-west-1.compute.internal Ready master 2d22h v1.13.0 EOD","title":"Testing cluster access"}]}